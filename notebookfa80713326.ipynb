{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30804,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"IMAGES_PATH = os.path.join('data','images')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.130381Z","iopub.status.idle":"2024-12-16T08:34:18.130671Z","shell.execute_reply.started":"2024-12-16T08:34:18.130521Z","shell.execute_reply":"2024-12-16T08:34:18.130536Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport os\nimport uuid\nimport time\nimport zipfile\nfrom IPython.display import FileLink, display\nfrom PIL import Image\n\n# Path to store the captured images\nIMAGES_PATH = 'captured_images'  # Save images in the current working directory\n\n# Number of images you want to capture\nnumber_images = 90  # Set this to 90 for capturing 90 images\n\n# Initialize the video capture object to use the first camera (index 0)\ncap = cv2.VideoCapture(0)\n\n# Check if the camera is opened successfully\nif not cap.isOpened():\n    print(\"Error: Could not open camera.\")\n    exit()\n\n# Create the directory to save images if it doesn't exist\nos.makedirs(IMAGES_PATH, exist_ok=True)\n\n# Loop to collect images based on the specified number of images (number_images)\nfor imgnum in range(number_images):\n    print(f'Collecting image {imgnum}')\n    \n    # Capture a frame from the video feed\n    ret, frame = cap.read()\n    \n    # Check if the frame was captured successfully\n    if not ret or frame is None:\n        print(\"Error: Failed to capture image.\")\n        break\n    \n    # Generate a unique image filename using UUID, and save the captured frame\n    imgname = os.path.join(IMAGES_PATH, f'{str(uuid.uuid1())}.jpg')\n    cv2.imwrite(imgname, frame)\n\n    # Display the captured frame in the notebook using PIL for better display\n    img_display = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n    display(img_display)\n    \n    # Add a small delay of 0.5 seconds before capturing the next image\n    time.sleep(0.5)\n\n# Release the video capture object\ncap.release()\n\n# Create a zip file of all captured images\nzip_filename = 'captured_images.zip'\nwith zipfile.ZipFile(zip_filename, 'w') as zipf:\n    for img_file in os.listdir(IMAGES_PATH):\n        img_path = os.path.join(IMAGES_PATH, img_file)\n        zipf.write(img_path, os.path.basename(img_path))\n\n# Print the path to the zip file\nprint(f'Images saved and zipped into {zip_filename}')\n\n# Provide a download link for the zip file in Jupyter\ndisplay(FileLink(zip_filename))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.133319Z","iopub.status.idle":"2024-12-16T08:34:18.133755Z","shell.execute_reply.started":"2024-12-16T08:34:18.133570Z","shell.execute_reply":"2024-12-16T08:34:18.133588Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Annotate Images with LabelMe","metadata":{}},{"cell_type":"code","source":"!labelme","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.135725Z","iopub.status.idle":"2024-12-16T08:34:18.136172Z","shell.execute_reply.started":"2024-12-16T08:34:18.135873Z","shell.execute_reply":"2024-12-16T08:34:18.135888Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  Load Image into TF Data Pipeline","metadata":{}},{"cell_type":"code","source":"images = tf.data.Dataset.list_files(r'C:\\Users\\DELL\\Downloads\\data\\images\\*.jpg')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.139484Z","iopub.status.idle":"2024-12-16T08:34:18.139800Z","shell.execute_reply.started":"2024-12-16T08:34:18.139647Z","shell.execute_reply":"2024-12-16T08:34:18.139663Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images.as_numpy_iterator().next()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.140868Z","iopub.status.idle":"2024-12-16T08:34:18.143385Z","shell.execute_reply.started":"2024-12-16T08:34:18.143086Z","shell.execute_reply":"2024-12-16T08:34:18.143115Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_image(x):\n    # Read the file at the path specified by 'x' into a byte string\n    byte_img = tf.io.read_file(x)\n    \n    # Decode the byte string into a JPEG image\n    # This converts the raw bytes into a usable tensor representing the image\n    img = tf.io.decode_jpeg(byte_img)\n    \n    return img\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.144518Z","iopub.status.idle":"2024-12-16T08:34:18.146456Z","shell.execute_reply.started":"2024-12-16T08:34:18.146270Z","shell.execute_reply":"2024-12-16T08:34:18.146291Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images = images.map(load_image)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.149572Z","iopub.status.idle":"2024-12-16T08:34:18.150528Z","shell.execute_reply.started":"2024-12-16T08:34:18.150326Z","shell.execute_reply":"2024-12-16T08:34:18.150347Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#retrieve the first element from a TensorFlow tf.data.Dataset as a NumPy array\nimages.as_numpy_iterator().next()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.151755Z","iopub.status.idle":"2024-12-16T08:34:18.154269Z","shell.execute_reply.started":"2024-12-16T08:34:18.154060Z","shell.execute_reply":"2024-12-16T08:34:18.154083Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"type(images)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.154907Z","iopub.status.idle":"2024-12-16T08:34:18.155235Z","shell.execute_reply.started":"2024-12-16T08:34:18.155080Z","shell.execute_reply":"2024-12-16T08:34:18.155097Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_generator = images.batch(4).as_numpy_iterator()\nplot_images = image_generator.next()\nfig, ax = plt.subplots(ncols=4, figsize=(20,20))\nfor idx, image in enumerate(plot_images):\n    ax[idx].imshow(image) \nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.155777Z","iopub.status.idle":"2024-12-16T08:34:18.158173Z","shell.execute_reply.started":"2024-12-16T08:34:18.155925Z","shell.execute_reply":"2024-12-16T08:34:18.155940Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# SPLIT DATA INTO TRAIN TEST AND VAL","metadata":{}},{"cell_type":"code","source":"# 64 to train\n# 17 and 17 to test and val\n\nimport os\n\nbase_path = r'C:\\Users\\DELL\\Downloads\\data'\n\n# List images and labels in the respective folders\nimage_folder = os.path.join(base_path, 'images')  # All images are in the images folder\nlabel_folder = os.path.join(base_path, 'labels')  # All labels are in the labels folder\n\nimage_files = os.listdir(image_folder)\nlabel_files = os.listdir(label_folder)\n\nprint(\"Image Files:\", image_files)\nprint(\"Label Files:\", label_files)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.159029Z","iopub.status.idle":"2024-12-16T08:34:18.159335Z","shell.execute_reply.started":"2024-12-16T08:34:18.159180Z","shell.execute_reply":"2024-12-16T08:34:18.159195Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Splitting 90 images into datasets:\n# 90 * 0.7 = 63 images for training\n# 90 * 0.15 = 14 images for testing\n# 90 * 0.15 = 13 images for validation\nimport os\nbase_path = r'C:\\Users\\DELL\\Downloads\\data'\n\nfor folder in ['train', 'test', 'val']:\n    image_folder = os.path.join(base_path, folder, 'images')\n    label_folder = os.path.join(base_path, folder, 'labels')\n    \n    # Ensure the subfolders exist for both images and labels\n    os.makedirs(image_folder, exist_ok=True)\n    os.makedirs(label_folder, exist_ok=True)\n    \n    # Get all image files in the corresponding dataset folder\n    for image_file in os.listdir(image_folder):\n        # Extract the base filename and check for corresponding label\n        image_base_name = os.path.splitext(image_file)[0]\n        label_filename = image_base_name + '.json'\n        existing_label_filepath = os.path.join(base_path, 'labels', label_filename)\n        \n        # Debugging output to verify the matching process\n        print(f\"Checking for label for image: {image_file}\")\n        \n        # If the label file exists in the 'labels' folder, move it to the current folder's label folder\n        if os.path.exists(existing_label_filepath):\n            new_label_filepath = os.path.join(label_folder, label_filename)\n            os.replace(existing_label_filepath, new_label_filepath)\n            print(f\"Moved label {label_filename} to {label_folder}\")\n        else:\n            print(f\"No label found for image: {image_file}\")\n\n            \n#C:\\Users\\DELL\\Downloads\\data\n#    ├── images               # Original images folder (unchanged)\n#    ├── labels               # Original labels folder (unchanged)\n#    ├── train\n#    │   ├── images           # Training images\n#    │   └── labels           # Training labels\n#    ├── test\n#    │   ├── images           # Test images\n#    │   └── labels           # Test labels\n#    └── val\n#        ├── images           # Validation images\n#        └── labels           # Validation labels\n            \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.159933Z","iopub.status.idle":"2024-12-16T08:34:18.162329Z","shell.execute_reply.started":"2024-12-16T08:34:18.162159Z","shell.execute_reply":"2024-12-16T08:34:18.162179Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Apply Image Augmentation on Images and Labels using Albumentations","metadata":{}},{"cell_type":"code","source":"import albumentations as alb\n\n# Define a data augmentation pipeline using Albumentations\naugmentor = alb.Compose([\n    # Randomly crop the image to 450x450 pixels\n    alb.RandomCrop(width=450, height=450),  \n    \n    # Apply a horizontal flip with a 50% probability\n    alb.HorizontalFlip(p=0.5),  \n    \n    # Adjust brightness and contrast with a 20% probability\n    alb.RandomBrightnessContrast(p=0.2),  \n    \n    # Apply random gamma correction with a 20% probability\n    alb.RandomGamma(p=0.2),  \n    \n    # Shift the RGB channels (e.g., change color tones) with a 20% probability\n    alb.RGBShift(p=0.2),  \n    \n    # Apply a vertical flip with a 50% probability\n    alb.VerticalFlip(p=0.5)  \n], \n# Specify parameters for bounding box processing\nbbox_params=alb.BboxParams(\n    format='albumentations',  # Bounding box format is Albumentations (x_min, y_min, x_max, y_max)\n    label_fields=['class_labels']  # The labels associated with each bounding box\n))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.162940Z","iopub.status.idle":"2024-12-16T08:34:18.163270Z","shell.execute_reply.started":"2024-12-16T08:34:18.163110Z","shell.execute_reply":"2024-12-16T08:34:18.163126Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Load a Test Image and Annotation with OpenCV and JSON","metadata":{}},{"cell_type":"code","source":"import os\n\n# Define the correct folder path\nlabel_folder_path = r'C:\\Users\\DELL\\Downloads\\data\\train\\labels'\n\n# Check if the 'labels' folder exists\nif os.path.exists(label_folder_path):\n    print(f\"Directory exists: {label_folder_path}\")\nelse:\n    print(f\"Directory does not exist: {label_folder_path}\")\n\n# List the files in the directory if it exists\nif os.path.exists(label_folder_path):\n    label_files = os.listdir(label_folder_path)\n    print(\"Files in labels folder:\", label_files)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.163825Z","iopub.status.idle":"2024-12-16T08:34:18.166217Z","shell.execute_reply.started":"2024-12-16T08:34:18.166010Z","shell.execute_reply":"2024-12-16T08:34:18.166037Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport cv2\n\n# Define the paths to your image and label file\nimage_path = r'C:\\Users\\DELL\\Downloads\\data\\train\\images\\c55065e4-b992-11ef-8e5b-c03eba00094a.jpg'\nlabel_file_path = r'C:\\Users\\DELL\\Downloads\\data\\train\\labels\\c55065e4-b992-11ef-8e5b-c03eba00094a.json'\n\n# Load the image using OpenCV\nimg = cv2.imread(image_path)\n\n# Load the label file\nwith open(label_file_path, 'r') as f:\n    label = json.load(f)\n\n# Access label points (e.g., the first shape's points)\nprint(label['shapes'][0]['points'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.168419Z","iopub.status.idle":"2024-12-16T08:34:18.168713Z","shell.execute_reply.started":"2024-12-16T08:34:18.168563Z","shell.execute_reply":"2024-12-16T08:34:18.168579Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images_dir = r'C:\\Users\\DELL\\data\\train\\images'  # Full path to the images directory\nlabels_dir = r'C:\\Users\\DELL\\data\\train\\labels'  # Full path to the labels directory\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.169275Z","iopub.status.idle":"2024-12-16T08:34:18.169561Z","shell.execute_reply.started":"2024-12-16T08:34:18.169416Z","shell.execute_reply":"2024-12-16T08:34:18.169431Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###  Extract Coordinates and Rescale to Match Image Resolution","metadata":{}},{"cell_type":"code","source":"# Initialize a list to store the coordinates\ncoords = [0, 0, 0, 0]\n\n# Assign the x-coordinate of the first point from the label to coords[0]\ncoords[0] = label['shapes'][0]['points'][0][0]\n\n# Assign the y-coordinate of the first point from the label to coords[1]\ncoords[1] = label['shapes'][0]['points'][0][1]\n\n# Assign the x-coordinate of the second point from the label to coords[2]\ncoords[2] = label['shapes'][0]['points'][1][0]\n\n# Assign the y-coordinate of the second point from the label to coords[3]\ncoords[3] = label['shapes'][0]['points'][1][1]\ncoords","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.172213Z","iopub.status.idle":"2024-12-16T08:34:18.173477Z","shell.execute_reply.started":"2024-12-16T08:34:18.173290Z","shell.execute_reply":"2024-12-16T08:34:18.173310Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"coords = list(np.divide(coords, [640,480,640,480]))\ncoords","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.176467Z","iopub.status.idle":"2024-12-16T08:34:18.176781Z","shell.execute_reply.started":"2024-12-16T08:34:18.176628Z","shell.execute_reply":"2024-12-16T08:34:18.176643Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Apply Augmentations and View Results","metadata":{}},{"cell_type":"code","source":"augmented = augmentor(image=img, bboxes=[coords], class_labels=['face'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.178009Z","iopub.status.idle":"2024-12-16T08:34:18.178336Z","shell.execute_reply.started":"2024-12-16T08:34:18.178180Z","shell.execute_reply":"2024-12-16T08:34:18.178197Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(type(img))  # Should be <class 'numpy.ndarray'>\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.181352Z","iopub.status.idle":"2024-12-16T08:34:18.181670Z","shell.execute_reply.started":"2024-12-16T08:34:18.181515Z","shell.execute_reply":"2024-12-16T08:34:18.181531Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"augmented['bboxes'][0][2:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.182883Z","iopub.status.idle":"2024-12-16T08:34:18.183228Z","shell.execute_reply.started":"2024-12-16T08:34:18.183072Z","shell.execute_reply":"2024-12-16T08:34:18.183089Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"augmented['bboxes']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.187644Z","iopub.status.idle":"2024-12-16T08:34:18.188260Z","shell.execute_reply.started":"2024-12-16T08:34:18.187943Z","shell.execute_reply":"2024-12-16T08:34:18.187992Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cv2.rectangle(augmented['image'], \n              tuple(np.multiply(augmented['bboxes'][0][:2], [450,450]).astype(int)),\n              tuple(np.multiply(augmented['bboxes'][0][2:], [450,450]).astype(int)), \n                    (255,0,0), 2)\n\nplt.imshow(augmented['image'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.192599Z","iopub.status.idle":"2024-12-16T08:34:18.193209Z","shell.execute_reply.started":"2024-12-16T08:34:18.192935Z","shell.execute_reply":"2024-12-16T08:34:18.192983Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Build and Run Augmentation Pipeline","metadata":{}},{"cell_type":"code","source":"# Define the base paths for the dataset and the augmented data\nbase_path = r'C:\\Users\\DELL\\Downloads\\data'  # Base path for input data (train, test, val folders)\naugmented_path = r'C:\\Users\\DELL\\Downloads\\aug_data'  # Base path for output augmented data\n\n# Loop through each data partition (train, test, val) to process images and labels\nfor partition in ['train', 'test', 'val']: \n    # Define paths for images and labels within the current partition\n    images_path = os.path.join(base_path, partition, 'images')  # Path to the images directory\n    labels_path = os.path.join(base_path, partition, 'labels')  # Path to the labels directory\n    \n    # Define paths to store the augmented images and labels for the current partition\n    augmented_images_path = os.path.join(augmented_path, partition, 'images')  \n    augmented_labels_path = os.path.join(augmented_path, partition, 'labels')  \n    \n    # Create directories for augmented images and labels if they do not exist\n    os.makedirs(augmented_images_path, exist_ok=True)\n    os.makedirs(augmented_labels_path, exist_ok=True)\n\n    # Loop through all images in the current partition\n    for image in os.listdir(images_path):\n        # Load the image from the images directory\n        img = cv2.imread(os.path.join(images_path, image))  \n\n        # Default bbox coordinates in case no label file exists for the image\n        coords = [0, 0, 0.00001, 0.00001]  \n        \n        # Define the path to the corresponding label file (JSON) for the current image\n        label_path = os.path.join(labels_path, f'{image.split(\".\")[0]}.json')  \n\n        # Check if the label file exists for the current image\n        if os.path.exists(label_path):\n            with open(label_path, 'r') as f:  # Open and read the label file\n                label = json.load(f)  # Load the JSON file into a Python dictionary\n\n            # Extract x and y coordinates from the label points and sort them to ensure correct order\n            x_coords = sorted([label['shapes'][0]['points'][0][0], label['shapes'][0]['points'][1][0]])\n            y_coords = sorted([label['shapes'][0]['points'][0][1], label['shapes'][0]['points'][1][1]])\n            coords = [x_coords[0], y_coords[0], x_coords[1], y_coords[1]]  # [x_min, y_min, x_max, y_max]\n\n            # Normalize the bounding box coordinates with respect to the image dimensions\n            h, w = img.shape[:2]  # Extract height (h) and width (w) of the image\n            coords = list(np.divide(coords, [w, h, w, h]))  # Normalize to be relative to image size\n\n        try: \n            # Augment each image 60 times\n            for x in range(60):  \n                # Apply data augmentation to the image and its corresponding bounding box\n                augmented = augmentor(image=img, bboxes=[coords], class_labels=['face'])\n                \n                # Save the augmented image with a unique filename\n                aug_img_name = f'{image.split(\".\")[0]}.{x}.jpg'  # Generate unique filename for each augmentation\n                cv2.imwrite(os.path.join(augmented_images_path, aug_img_name), augmented['image'])  # Save image\n\n                # Create an annotation file for the augmented image\n                annotation = {}  \n                annotation['image'] = aug_img_name  # Store the name of the augmented image\n\n                # Check if the label file exists for the image and if a valid bounding box was found after augmentation\n                if os.path.exists(label_path):\n                    if len(augmented['bboxes']) == 0:  # No bounding box was detected after augmentation\n                        annotation['bbox'] = [0, 0, 0, 0]  # Default bbox if no face is detected\n                        annotation['class'] = 0  # No face detected\n                    else:  \n                        annotation['bbox'] = augmented['bboxes'][0]  # Store the new bounding box after augmentation\n                        annotation['class'] = 1  # Face detected\n                else:  \n                    annotation['bbox'] = [0, 0, 0, 0]  # No label file means no face detected\n                    annotation['class'] = 0  # No face detected\n\n                # Save the annotation as a JSON file corresponding to the augmented image\n                aug_label_name = f'{image.split(\".\")[0]}.{x}.json'  # Unique filename for each annotation\n                with open(os.path.join(augmented_labels_path, aug_label_name), 'w') as f:  \n                    json.dump(annotation, f)  # Write the annotation to the JSON file\n\n        except Exception as e:\n            print(f\"Error with image {image}: {e}\")  # Print the error message if an exception occurs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.194495Z","iopub.status.idle":"2024-12-16T08:34:18.195620Z","shell.execute_reply.started":"2024-12-16T08:34:18.195420Z","shell.execute_reply":"2024-12-16T08:34:18.195443Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Load Augmented Images to Tensorflow Dataset","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\n# Paths to the image directories for train, test, and validation sets\ntrain_images_path = r'C:\\Users\\DELL\\Downloads\\aug_data\\train\\images\\*.jpg'\ntest_images_path = r'C:\\Users\\DELL\\Downloads\\aug_data\\test\\images\\*.jpg'\nval_images_path = r'C:\\Users\\DELL\\Downloads\\aug_data\\val\\images\\*.jpg'\n\n# Function to load and preprocess images\ndef load_and_preprocess_images(path):\n    images = tf.data.Dataset.list_files(path, shuffle=False)  # List all image files in the specified path\n    images = images.map(load_image)  # Load each image using the load_image function\n    images = images.map(lambda x: tf.image.resize(x, (120, 120)))  # Resize images to 120x120 pixels\n    images = images.map(lambda x: x / 255)  # Normalize image pixel values to the range [0, 1]\n    return images\n\n# Load and preprocess images for train, test, and validation datasets\ntrain_images = load_and_preprocess_images(train_images_path)\ntest_images = load_and_preprocess_images(test_images_path)\nval_images = load_and_preprocess_images(val_images_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.198516Z","iopub.status.idle":"2024-12-16T08:34:18.198827Z","shell.execute_reply.started":"2024-12-16T08:34:18.198672Z","shell.execute_reply":"2024-12-16T08:34:18.198688Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_images.as_numpy_iterator().next()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.200190Z","iopub.status.idle":"2024-12-16T08:34:18.200846Z","shell.execute_reply.started":"2024-12-16T08:34:18.200665Z","shell.execute_reply":"2024-12-16T08:34:18.200685Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Prepare Labels","metadata":{}},{"cell_type":"code","source":"#Build Label Loading Function\ndef load_labels(label_path):\n    with open(label_path.numpy(), 'r', encoding = \"utf-8\") as f:\n        label = json.load(f)\n        \n    return [label['class']], label['bbox']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.203542Z","iopub.status.idle":"2024-12-16T08:34:18.204185Z","shell.execute_reply.started":"2024-12-16T08:34:18.204009Z","shell.execute_reply":"2024-12-16T08:34:18.204029Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n\n# Paths to the image directories for train, test, and validation sets\ntrain_images_path = r'C:\\Users\\DELL\\Downloads\\aug_data\\train\\images\\*.jpg'\ntest_images_path = r'C:\\Users\\DELL\\Downloads\\aug_data\\test\\images\\*.jpg'\nval_images_path = r'C:\\Users\\DELL\\Downloads\\aug_data\\val\\images\\*.jpg'\n\n# Paths to the label directories for train, test, and validation sets\ntrain_labels_path = r'C:\\Users\\DELL\\Downloads\\aug_data\\train\\labels\\*.json'\ntest_labels_path = r'C:\\Users\\DELL\\Downloads\\aug_data\\test\\labels\\*.json'\nval_labels_path = r'C:\\Users\\DELL\\Downloads\\aug_data\\val\\labels\\*.json'\n\n# Function to load and preprocess images\ndef load_and_preprocess_images(path):\n    images = tf.data.Dataset.list_files(path, shuffle=False)  # List all image files in the specified path\n    images = images.map(load_image)  # Load each image using the load_image function\n    images = images.map(lambda x: tf.image.resize(x, (120, 120)))  # Resize images to 120x120 pixels\n    images = images.map(lambda x: x / 255)  # Normalize image pixel values to the range [0, 1]\n    return images\n\n# Load and preprocess images for train, test, and validation datasets\ntrain_images = load_and_preprocess_images(train_images_path)\ntest_images = load_and_preprocess_images(test_images_path)\nval_images = load_and_preprocess_images(val_images_path)\n\n# Load labels to TensorFlow datasets\ndef load_and_preprocess_labels(path):\n    labels = tf.data.Dataset.list_files(path, shuffle=False)  # List all label files in the specified path\n    labels = labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))  # Load labels using the load_labels function\n    return labels\n\n# Load and preprocess labels for train, test, and validation datasets\ntrain_labels = load_and_preprocess_labels(train_labels_path)\ntest_labels = load_and_preprocess_labels(test_labels_path)\nval_labels = load_and_preprocess_labels(val_labels_path)\n\n# Check first label from the train dataset\nprint(train_labels.as_numpy_iterator().next())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.205146Z","iopub.status.idle":"2024-12-16T08:34:18.208271Z","shell.execute_reply.started":"2024-12-16T08:34:18.208060Z","shell.execute_reply":"2024-12-16T08:34:18.208083Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Combine Label and Image Samples","metadata":{}},{"cell_type":"code","source":"\n# Print the lengths of the datasets\nprint(\"Number of train images:\", len(list(train_images)))\nprint(\"Number of train labels:\", len(list(train_labels)))\nprint(\"Number of test images:\", len(list(test_images)))\nprint(\"Number of test labels:\", len(list(test_labels)))\nprint(\"Number of validation images:\", len(list(val_images)))\nprint(\"Number of validation labels:\", len(list(val_labels)))\n\n# Create Final Datasets (Images/Labels)\ntrain = tf.data.Dataset.zip((train_images, train_labels))  # Combine train images and labels into a single dataset\ntrain = train.shuffle(5000)  # Shuffle the dataset with a buffer size of 5000\ntrain = train.batch(8)  # Batch the dataset with a batch size of 8\ntrain = train.prefetch(4)  # Prefetch 4 batches for optimal performance\n\ntest = tf.data.Dataset.zip((test_images, test_labels))  # Combine test images and labels into a single dataset\ntest = test.shuffle(1300)  # Shuffle the dataset with a buffer size of 1300\ntest = test.batch(8)  # Batch the dataset with a batch size of 8\ntest = test.prefetch(4)  # Prefetch 4 batches for optimal performance\n\nval = tf.data.Dataset.zip((val_images, val_labels))  # Combine validation images and labels into a single dataset\nval = val.shuffle(1000)  # Shuffle the dataset with a buffer size of 1000\nval = val.batch(8)  # Batch the dataset with a batch size of 8\nval = val.prefetch(4)  # Prefetch 4 batches for optimal performance\n\n# Check first label from the train dataset\nprint(train.as_numpy_iterator().next()[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.208917Z","iopub.status.idle":"2024-12-16T08:34:18.209246Z","shell.execute_reply.started":"2024-12-16T08:34:18.209090Z","shell.execute_reply":"2024-12-16T08:34:18.209107Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### View Images and Annotations","metadata":{}},{"cell_type":"code","source":"# Visualize some samples from the train dataset\ndata_samples = train.as_numpy_iterator()  # Create an iterator for the train dataset\nres = data_samples.next()  # Get the next batch of data\n\n# Create a plot with 4 columns to visualize 4 images and their bounding boxes\nfig, ax = plt.subplots(ncols=4, figsize=(20, 20))\n\nfor idx in range(4): \n    sample_image = res[0][idx].copy()  # Make a writable copy of the image\n    sample_coords = res[1][1][idx]  # Extract the bounding box coordinates from the batch\n    \n    # Draw a rectangle around the detected face using the bounding box coordinates\n    cv2.rectangle(sample_image, \n                  tuple(np.multiply(sample_coords[:2], [120, 120]).astype(int)),  # Top-left corner of the bounding box\n                  tuple(np.multiply(sample_coords[2:], [120, 120]).astype(int)),  # Bottom-right corner of the bounding box\n                  (255, 0, 0), 2)  # Color (red) and thickness of the rectangle\n\n    ax[idx].imshow(sample_image)  # Display the image with the bounding box on the subplot\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.209827Z","iopub.status.idle":"2024-12-16T08:34:18.210154Z","shell.execute_reply.started":"2024-12-16T08:34:18.209989Z","shell.execute_reply":"2024-12-16T08:34:18.210015Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Build Deep Learning using the Functional API","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Conv2D, Dense, GlobalMaxPooling2D\nfrom tensorflow.keras.applications import VGG16","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.210687Z","iopub.status.idle":"2024-12-16T08:34:18.213025Z","shell.execute_reply.started":"2024-12-16T08:34:18.210820Z","shell.execute_reply":"2024-12-16T08:34:18.210835Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vgg = VGG16(include_top=False)\nvgg.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.215033Z","iopub.status.idle":"2024-12-16T08:34:18.215366Z","shell.execute_reply.started":"2024-12-16T08:34:18.215210Z","shell.execute_reply":"2024-12-16T08:34:18.215226Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###  Build instance of Network","metadata":{}},{"cell_type":"code","source":"def build_model(): \n    input_layer = Input(shape=(120, 120, 3))  # Input shape of 120x120 RGB image\n    \n    vgg_output = VGG16(include_top=False)(input_layer)  # Use VGG16 as feature extractor\n\n    # Classification Model  \n    f1 = GlobalMaxPooling2D()(vgg_output)  # Apply global max pooling\n    class1 = Dense(2048, activation='relu')(f1)  # Fully connected layer for classification\n    class2 = Dense(1, activation='sigmoid')(class1)  # Output layer for binary classification (face/no face)\n    \n    # Bounding box model\n    f2 = GlobalMaxPooling2D()(vgg_output)  # Apply global max pooling for bbox\n    regress1 = Dense(2048, activation='relu')(f2)  # Fully connected layer for bounding box prediction\n    regress2 = Dense(4, activation='sigmoid')(regress1)  # Output layer for bounding box (x1, y1, x2, y2)\n    \n    facetracker = Model(inputs=input_layer, outputs=[class2, regress2])  # Define the model with two outputs\n    return facetracker\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.218509Z","iopub.status.idle":"2024-12-16T08:34:18.218832Z","shell.execute_reply.started":"2024-12-16T08:34:18.218674Z","shell.execute_reply":"2024-12-16T08:34:18.218690Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Test out Neural Network","metadata":{}},{"cell_type":"code","source":"facetracker = build_model()  # Build the model\nfacetracker.summary()  # Print the model summary\n\n# Test prediction\nX, y = train.as_numpy_iterator().next()  # Get a batch from the training data\nX.shape  # Check the shape of input data\nclasses, coords = facetracker.predict(X)  # Get predictions\nprint(classes, coords)  # Print the predictions\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.220206Z","iopub.status.idle":"2024-12-16T08:34:18.221070Z","shell.execute_reply.started":"2024-12-16T08:34:18.220856Z","shell.execute_reply":"2024-12-16T08:34:18.220876Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Define Losses and Optimizers","metadata":{}},{"cell_type":"code","source":"# 9.1 Define Optimizer and Learning Rate (LR) Decay\nbatches_per_epoch = len(train)\nlr_decay = (1. / 0.75 - 1) / batches_per_epoch  # Learning rate decay formula\nopt = tf.keras.optimizers.Adam(learning_rate=0.0001, decay=lr_decay)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.224561Z","iopub.status.idle":"2024-12-16T08:34:18.225305Z","shell.execute_reply.started":"2024-12-16T08:34:18.225122Z","shell.execute_reply":"2024-12-16T08:34:18.225142Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train Neural Network","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.layers import Input, GlobalMaxPooling2D, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.losses import BinaryCrossentropy\nimport json\nimport glob\n\n# Paths to the image and label directories for train, test, and validation sets\ntrain_images_path = r'C:\\Users\\DELL\\Downloads\\aug_data\\train\\images\\*.jpg'\ntest_images_path = r'C:\\Users\\DELL\\Downloads\\aug_data\\test\\images\\*.jpg'\nval_images_path = r'C:\\Users\\DELL\\Downloads\\aug_data\\val\\images\\*.jpg'\n\ntrain_labels_path = glob.glob(r'C:\\Users\\DELL\\Downloads\\aug_data\\train\\labels\\*.json')\ntest_labels_path = glob.glob(r'C:\\Users\\DELL\\Downloads\\aug_data\\test\\labels\\*.json')\nval_labels_path = glob.glob(r'C:\\Users\\DELL\\Downloads\\aug_data\\val\\labels\\*.json')\n\n# Helper function to load and preprocess images\ndef load_and_preprocess_image(file_path):\n    image = tf.io.read_file(file_path)\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.resize(image, (120, 120))\n    image = image / 255.0\n    return image\n\n# Helper function to load labels\ndef load_labels(label_path):\n    with open(label_path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    class_label = data['class']  # Binary classification label (0 or 1)\n    bbox = data['bbox']  # Bounding box coordinates [x1, y1, x2, y2]\n    return class_label, bbox\n\n# Create a TensorFlow dataset for images and labels\ndef create_dataset(image_paths, label_paths):\n    images = [load_and_preprocess_image(img) for img in image_paths]\n    labels = [load_labels(lbl) for lbl in label_paths]\n    class_labels = [lbl[0] for lbl in labels]\n    bboxes = [lbl[1] for lbl in labels]\n    dataset = tf.data.Dataset.from_tensor_slices((images, (class_labels, bboxes)))\n    dataset = dataset.shuffle(500).batch(8).prefetch(tf.data.AUTOTUNE)\n    return dataset\n\n# Prepare datasets\ntrain_image_paths = glob.glob(train_images_path)\ntest_image_paths = glob.glob(test_images_path)\nval_image_paths = glob.glob(val_images_path)\n\ntrain_dataset = create_dataset(train_image_paths, train_labels_path)\ntest_dataset = create_dataset(test_image_paths, test_labels_path)\nval_dataset = create_dataset(val_image_paths, val_labels_path)\n\n# Build the model\ndef build_model():\n    input_layer = Input(shape=(120, 120, 3))\n    base_model = VGG16(include_top=False, input_tensor=input_layer)\n    \n    # Classification head\n    x = GlobalMaxPooling2D()(base_model.output)\n    class_head = Dense(2048, activation='relu')(x)\n    class_output = Dense(1, activation='sigmoid', name='class_output')(class_head)\n    \n    # Regression head\n    y = GlobalMaxPooling2D()(base_model.output)\n    bbox_head = Dense(2048, activation='relu')(y)\n    bbox_output = Dense(4, activation='sigmoid', name='bbox_output')(bbox_head)\n    \n    model = Model(inputs=input_layer, outputs=[class_output, bbox_output])\n    return model\n\nfacetracker = build_model()\n\n# Custom loss functions\ndef localization_loss(y_true, y_pred):\n    delta_coord = tf.reduce_sum(tf.square(y_true[:, :2] - y_pred[:, :2]))\n    h_true = y_true[:, 3] - y_true[:, 1]\n    w_true = y_true[:, 2] - y_true[:, 0]\n    h_pred = y_pred[:, 3] - y_pred[:, 1]\n    w_pred = y_pred[:, 2] - y_pred[:, 0]\n    delta_size = tf.reduce_sum(tf.square(w_true - w_pred) + tf.square(h_true - h_pred))\n    return delta_coord + delta_size\n\nclass_loss = BinaryCrossentropy()\n\n# Compile the model\nfacetracker.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n    loss={'class_output': class_loss, 'bbox_output': localization_loss},\n    loss_weights={'class_output': 0.5, 'bbox_output': 1.0},\n    metrics={'class_output': 'accuracy'}\n)\n\n# Train the model\nfacetracker.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=10\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.225993Z","iopub.status.idle":"2024-12-16T08:34:18.226305Z","shell.execute_reply.started":"2024-12-16T08:34:18.226153Z","shell.execute_reply":"2024-12-16T08:34:18.226169Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Make Predictions","metadata":{}},{"cell_type":"code","source":"def rescale_bboxes(coords, original_shape):\n    height, width = original_shape[:2]\n    x1, y1, x2, y2 = coords\n    return int(x1 * width), int(y1 * height), int(x2 * width), int(y2 * height)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.226869Z","iopub.status.idle":"2024-12-16T08:34:18.229298Z","shell.execute_reply.started":"2024-12-16T08:34:18.229091Z","shell.execute_reply":"2024-12-16T08:34:18.229112Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to make predictions on a single image\ndef predict_and_draw(image_path, model):\n    # Load and preprocess the image\n    image = tf.io.read_file(image_path)\n    image = tf.image.decode_jpeg(image, channels=3)\n    original_shape = image.shape  # Store the original shape for scaling\n    image_resized = tf.image.resize(image, (120, 120)) / 255.0\n    image_resized = tf.expand_dims(image_resized, axis=0)  # Add batch dimension\n\n    # Get predictions from the model\n    class_pred, bbox_pred = model.predict(image_resized)\n    class_pred = class_pred[0][0]  # Extract the first batch prediction\n    bbox_pred = bbox_pred[0]  # Extract the first batch bounding box\n\n    # Rescale bounding box to original image size\n    x1, y1, x2, y2 = rescale_bboxes(bbox_pred, original_shape)\n\n    # Draw bounding box on the original image\n    image_with_bbox = cv2.cvtColor(image.numpy(), cv2.COLOR_RGB2BGR)\n    if class_pred > 0.5:  # If class confidence is high enough\n        cv2.rectangle(image_with_bbox, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Draw a green box\n\n    # Plot the image with bounding box\n    plt.imshow(cv2.cvtColor(image_with_bbox, cv2.COLOR_BGR2RGB))\n    plt.title(f\"Face: {'Yes' if class_pred > 0.5 else 'No'} | Confidence: {class_pred:.2f}\")\n    plt.axis('off')\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.229945Z","iopub.status.idle":"2024-12-16T08:34:18.230276Z","shell.execute_reply.started":"2024-12-16T08:34:18.230120Z","shell.execute_reply":"2024-12-16T08:34:18.230136Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_image_path = r\"C:\\Users\\DELL\\Downloads\\WhatsApp Image 2024-12-15 at 21.38.11_934a5c03.jpg\"\npredict_and_draw(test_image_path, facetracker)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.230822Z","iopub.status.idle":"2024-12-16T08:34:18.233252Z","shell.execute_reply.started":"2024-12-16T08:34:18.233019Z","shell.execute_reply":"2024-12-16T08:34:18.233050Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#\"C:\\Users\\DELL\\Downloads\\WhatsApp Image 2024-11-20 at 20.27.07_5bfadcd2.jpg\"\ntest_image_path = r\"C:\\Users\\DELL\\Downloads\\WhatsApp Image 2024-12-15 at 21.44.58_9a98589c.jpg\"\npredict_and_draw(test_image_path, facetracker)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.235355Z","iopub.status.idle":"2024-12-16T08:34:18.235694Z","shell.execute_reply.started":"2024-12-16T08:34:18.235536Z","shell.execute_reply":"2024-12-16T08:34:18.235552Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Save the Model","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nfacetracker.save('facetracker.h5')\nfacetracker = load_model('facetracker.h5')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.239439Z","iopub.status.idle":"2024-12-16T08:34:18.239820Z","shell.execute_reply.started":"2024-12-16T08:34:18.239645Z","shell.execute_reply":"2024-12-16T08:34:18.239662Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Real Time Detection","metadata":{}},{"cell_type":"code","source":"import cv2\nprint(cv2.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.240695Z","iopub.status.idle":"2024-12-16T08:34:18.241144Z","shell.execute_reply.started":"2024-12-16T08:34:18.240849Z","shell.execute_reply":"2024-12-16T08:34:18.240864Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ['OPENCV_WINDOW'] = 'Qt'  # or 'GTK'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.244386Z","iopub.status.idle":"2024-12-16T08:34:18.244730Z","shell.execute_reply.started":"2024-12-16T08:34:18.244568Z","shell.execute_reply":"2024-12-16T08:34:18.244584Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nprint(cv2.getBuildInformation())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.245364Z","iopub.status.idle":"2024-12-16T08:34:18.245666Z","shell.execute_reply.started":"2024-12-16T08:34:18.245514Z","shell.execute_reply":"2024-12-16T08:34:18.245529Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get the current working directory\ncurrent_dir = os.getcwd()\n\n# Modify the path based on your model location\nmodel_path = os.path.join(current_dir, 'models', 'your_model.h5')  # Adjust the path\n\n# Print the model path for verification\nprint(f\"Loading model from: {model_path}\")\n\n# ... (rest of your code)\n\n# Function to preprocess and make predictions\ndef predict_and_draw(frame):\n    # Preprocess the frame\n    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    image_resized = tf.image.resize(image, (120, 120)) / 255.0\n    image_resized = tf.expand_dims(image_resized, axis=0)\n\n    # Make predictions\n    yhat = model.predict(image_resized)\n    class_pred = yhat[0][0]\n    bbox_pred = yhat[1][0]\n\n    # Rescale bounding box to original frame size\n    x1, y1, x2, y2 = rescale_bboxes(bbox_pred, frame.shape[:2])\n\n    # Draw bounding box on the frame\n    if class_pred > 0.5:\n        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n\n    return frame\n\n# Start the video capture\ncap = cv2.VideoCapture(0)  # 0 for default camera\n\nwhile True:\n    ret, frame = cap.read()\n\n    if not ret:\n        print(\"Error: Unable to capture frame from webcam\")\n        break  # Exit the loop if there's an error\n\n    # Process the frame (preprocessing, prediction, drawing)\n    frame = predict_and_draw(frame)\n\n    # Display the frame using Matplotlib\n    plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n    plt.axis('off')\n    plt.show()\n    plt.clf()  # Clear the plot\n\n    # Exit on 'q' key press\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\n# Release resources\ncap.release()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:34:18.248327Z","iopub.status.idle":"2024-12-16T08:34:18.248675Z","shell.execute_reply.started":"2024-12-16T08:34:18.248508Z","shell.execute_reply":"2024-12-16T08:34:18.248524Z"}},"outputs":[],"execution_count":null}]}